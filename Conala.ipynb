{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensor2tensor[tensorflow_gpu]\n",
    "# !pip install pandas\n",
    "# !pip install sklearn\n",
    "# !pip install nltk\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensor2tensor import models\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.utils import t2t_model\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import metrics\n",
    "\n",
    "tfe = tf.contrib.eager\n",
    "tfe.enable_eager_execution()\n",
    "Modes = tf.estimator.ModeKeys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensor2tensor.data_generators import semantic_search\n",
    "search_problem = semantic_search.SemanticSearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_problem.generate_data(data_dir='/tf/tensor2tensor/t2t_data', tmp_dir='/tf/tensor2tensor/datagen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {}\n",
    "PARAMS['T2T_Problem'] = 'semantic_search'\n",
    "PARAMS['T2T_Model'] = 'transformer'\n",
    "PARAMS['T2T_HPARAMS'] = 'transformer_base'\n",
    "PARAMS['train_steps'] = 100000\n",
    "PARAMS['eval_steps'] = 100\n",
    "PARAMS['keep_checkpoint_max'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.utils.trainer_lib import create_run_config, create_experiment, create_hparams\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor import models, problems\n",
    "\n",
    "hparams = create_hparams(PARAMS['T2T_HPARAMS'])\n",
    "\n",
    "FLAGS = tf.flags\n",
    "FLAGS.problems = PARAMS['T2T_Problem']\n",
    "FLAGS.model = PARAMS['T2T_Model']\n",
    "FLAGS.schedule = \"train_and_evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS['TMP_DIR'] = '/tf/datagen/'\n",
    "PARAMS['DATA_DIR'] = '/tf/t2t_data'\n",
    "PARAMS['TRAIN_DIR'] = '/tf/t2t_train' \n",
    "PARAMS['OUTPUT_DIR'] = 'tf/t2t_train/semantic_search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CONFIG = create_run_config(hparams, model_dir=PARAMS['TRAIN_DIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_fn = create_experiment(\n",
    "        run_config=RUN_CONFIG,\n",
    "        hparams=hparams,\n",
    "        model_name=PARAMS['T2T_Model'],\n",
    "        problem_name=PARAMS['T2T_Problem'],\n",
    "        data_dir=PARAMS['DATA_DIR'],\n",
    "        train_steps=PARAMS['train_steps'],\n",
    "        eval_steps=PARAMS['eval_steps']\n",
    "    )\n",
    "exp_fn.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ckpt_path = tf.train.latest_checkpoint(PARAMS['TRAIN_DIR'])\n",
    "ckpt_path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting translatte model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_eval = trainer_lib.create_hparams(PARAMS['T2T_HPARAMS'], data_dir=PARAMS['DATA_DIR'], \n",
    "                                     problem_name=PARAMS['T2T_Problem'])\n",
    "\n",
    "translate_model = registry.model(PARAMS['T2T_Model'])(hparams_eval, Modes.EVAL)\n",
    "\n",
    "encoders = problems.problem(PARAMS['T2T_Problem']).feature_encoders(PARAMS['DATA_DIR'])\n",
    "\n",
    "def encode(input_str, output_str=None):\n",
    "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "    inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\n",
    "    batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\n",
    "    return {\"inputs\": batch_inputs}\n",
    "\n",
    "def decode(integers):\n",
    "    \"\"\"List of ints to str\"\"\"\n",
    "    integers = list(np.squeeze(integers))\n",
    "    if 1 in integers:\n",
    "        integers = integers[:integers.index(1)]\n",
    "    return encoders[\"inputs\"].decode(np.squeeze(integers))\n",
    "\n",
    "# Restore and translate!\n",
    "def translate(inputs):\n",
    "    encoded_inputs = encode(inputs)\n",
    "    with tfe.restore_variables_on_create(ckpt_path):\n",
    "        model_output = translate_model.infer(encoded_inputs)[\"outputs\"]\n",
    "        return decode(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conala_df = pd.read_json(\"/tf/datagen/conala-train.json\")\n",
    "conala_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent, code = conala_df.iloc[4].intent, conala_df.iloc[4].snippet\n",
    "print(intent)\n",
    "print(code)\n",
    "translate(code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
