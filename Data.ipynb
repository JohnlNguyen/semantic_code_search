{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from six import StringIO\n",
    "\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.utils import metrics\n",
    "from tensor2tensor.utils import registry\n",
    "\n",
    "\n",
    "@registry.register_problem\n",
    "class Conala(text_problems.Text2TextProblem):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def base_url(self):\n",
    "        return \"gs://conala\"\n",
    "\n",
    "    @property\n",
    "    def test_file(self):\n",
    "        return '{}/{}'.format(self.base_url, \"conala-test.json\"), \"conala-test.json\"\n",
    "\n",
    "    @property\n",
    "    def file_names(self):\n",
    "        return [\n",
    "            \"conala-mined.jsonl\",\n",
    "            \"conala-train.json\",\n",
    "            \"django-all.anno\",\n",
    "            \"django-all.code\"\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def pair_files_list(self):\n",
    "        \"\"\"\n",
    "        This function returns a list of (url, file name) pairs\n",
    "        \"\"\"\n",
    "        return [\n",
    "            ('{}/{}'.format(self.base_url, name),\n",
    "             name)\n",
    "            for name in self.file_names\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def is_generate_per_split(self):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def approx_vocab_size(self):\n",
    "        return 2 ** 13\n",
    "\n",
    "    @property\n",
    "    def max_samples_for_vocab(self):\n",
    "        return int(3.5e5)\n",
    "\n",
    "    def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "        \"\"\"A generator to return data samples.Returns the data generator to return.\n",
    "\n",
    "\n",
    "        Args:\n",
    "          data_dir: A string representing the data directory.\n",
    "          tmp_dir: A string representing the temporary directory and isÂ¬\n",
    "                  used to download files if not already available.\n",
    "          dataset_split: Train, Test or Eval.\n",
    "\n",
    "        Yields:\n",
    "          Each element yielded is of a Python dict of the form\n",
    "            {\"inputs\": \"STRING\", \"targets\": \"STRING\"}\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Manually separate train/eval data set.\n",
    "        file_names = self.pair_files_list\n",
    "        all_files = [\n",
    "            generator_utils.maybe_download(tmp_dir, file_name, uri)\n",
    "            for uri, file_name in file_names\n",
    "        ]\n",
    "\n",
    "        for file_name in all_files:\n",
    "            tf.logging.debug(\"Reading {}\".format(file_name))\n",
    "            if \".jsonl\" in file_name:\n",
    "                contents = Path(file_name).read_text()\n",
    "                contents = contents.splitlines()\n",
    "                df = pd.DataFrame([dict(eval(x)) for x in contents])\n",
    "                for row in df.iterrows():\n",
    "                    yield self.get_row_content(row)\n",
    "            elif \".json\" in file_name:\n",
    "                df = pd.read_json(file_name)\n",
    "                for row in df.iterrows():\n",
    "                    yield self.get_row_content(row)\n",
    "            else:\n",
    "                # TODO: Figure out how to handle django dataset\n",
    "                pass\n",
    "\n",
    "    def eval_metrics(self):\n",
    "        return [\n",
    "            metrics.Metrics.ACC,\n",
    "            metrics.Metrics.APPROX_BLEU\n",
    "        ]\n",
    "\n",
    "    def get_row_content(self, row):\n",
    "        if len(row) < 2:\n",
    "            raise Exception(\"Row does not have content\")\n",
    "        row = row[1]\n",
    "        return {\"inputs\": row.snippet,\n",
    "                \"targets\": row.rewritten_intent if 'rewritten_intent' in row and row.rewritten_intent != None else row.intent}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conala'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "tmp_dir = os.path.join(os.getcwd(), \"datagen\") \n",
    "\n",
    "c = Conala()\n",
    "c.dataset_filename()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
