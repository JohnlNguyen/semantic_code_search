{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Entry Point [tensor2tensor.envs.tic_tac_toe_env:TicTacToeEnv] registered with id [T2TEnv-TicTacToeEnv-v0]\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensor2tensor[tensorflow_gpu]\n",
    "#!pip install pandas\n",
    "#!pip install -e .\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensor2tensor import models\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.utils import t2t_model\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {}\n",
    "PARAMS['T2T_Problem'] = 'github_function_docstring'\n",
    "PARAMS['T2T_Model'] = 'transformer'\n",
    "PARAMS['T2T_HPARAMS'] = 'transformer_base'\n",
    "PARAMS['train_steps'] = 1001000\n",
    "PARAMS['eval_steps'] = 100\n",
    "PARAMS['keep_checkpoint_max'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.utils.trainer_lib import create_run_config, create_experiment, create_hparams\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor import models, problems\n",
    "\n",
    "hparams = create_hparams(PARAMS['T2T_HPARAMS'])\n",
    "\n",
    "\n",
    "FLAGS = tf.flags\n",
    "FLAGS.problems = PARAMS['T2T_Problem']\n",
    "FLAGS.model = PARAMS['T2T_Model']\n",
    "FLAGS.schedule = \"train_and_evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS['TMP_DIR'] = '/tf/datagen/github'\n",
    "PARAMS['DATA_DIR'] = '/tf/t2t_data'\n",
    "PARAMS['TRAIN_DIR'] = '/tf/t2t_train' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation_dtype': 'float32',\n",
       " 'add_relative_to_values': False,\n",
       " 'area_key_mode': 'none',\n",
       " 'area_value_mode': 'none',\n",
       " 'attention_dropout': 0.1,\n",
       " 'attention_dropout_broadcast_dims': '',\n",
       " 'attention_key_channels': 0,\n",
       " 'attention_value_channels': 0,\n",
       " 'attention_variables_3d': False,\n",
       " 'batch_shuffle_size': 512,\n",
       " 'batch_size': 4096,\n",
       " 'bottom': {},\n",
       " 'causal_decoder_self_attention': True,\n",
       " 'clip_grad_norm': 0.0,\n",
       " 'compress_steps': 0,\n",
       " 'conv_first_kernel': 3,\n",
       " 'daisy_chain_variables': True,\n",
       " 'dropout': 0.2,\n",
       " 'eval_drop_long_sequences': False,\n",
       " 'eval_run_autoregressive': False,\n",
       " 'factored_logits': False,\n",
       " 'ffn_layer': 'dense_relu_dense',\n",
       " 'filter_size': 2048,\n",
       " 'force_full_predict': False,\n",
       " 'grad_noise_scale': 0.0,\n",
       " 'hard_attention_k': 0,\n",
       " 'heads_share_relative_embedding': False,\n",
       " 'hidden_size': 512,\n",
       " 'initializer': 'uniform_unit_scaling',\n",
       " 'initializer_gain': 1.0,\n",
       " 'kernel_height': 3,\n",
       " 'kernel_width': 1,\n",
       " 'label_smoothing': 0.1,\n",
       " 'layer_postprocess_sequence': 'da',\n",
       " 'layer_prepostprocess_dropout': 0.1,\n",
       " 'layer_prepostprocess_dropout_broadcast_dims': '',\n",
       " 'layer_preprocess_sequence': 'n',\n",
       " 'learning_rate': 0.2,\n",
       " 'learning_rate_constant': 2.0,\n",
       " 'learning_rate_cosine_cycle_steps': 250000,\n",
       " 'learning_rate_decay_rate': 1.0,\n",
       " 'learning_rate_decay_scheme': 'noam',\n",
       " 'learning_rate_decay_staircase': False,\n",
       " 'learning_rate_decay_steps': 5000,\n",
       " 'learning_rate_minimum': None,\n",
       " 'learning_rate_schedule': 'constant*linear_warmup*rsqrt_decay*rsqrt_hidden_size',\n",
       " 'learning_rate_warmup_steps': 8000,\n",
       " 'length_bucket_step': 1.1,\n",
       " 'loss': {},\n",
       " 'max_area_height': 1,\n",
       " 'max_area_width': 1,\n",
       " 'max_input_seq_length': 0,\n",
       " 'max_length': 256,\n",
       " 'max_relative_position': 0,\n",
       " 'max_target_seq_length': 0,\n",
       " 'memory_height': 1,\n",
       " 'min_length': 0,\n",
       " 'min_length_bucket': 8,\n",
       " 'mixed_precision_optimizer_init_loss_scale': 32768,\n",
       " 'mixed_precision_optimizer_loss_scaler': 'exponential',\n",
       " 'mlperf_mode': False,\n",
       " 'moe_hidden_sizes': '2048',\n",
       " 'moe_k': 2,\n",
       " 'moe_loss_coef': 0.001,\n",
       " 'moe_num_experts': 16,\n",
       " 'moe_overhead_eval': 2.0,\n",
       " 'moe_overhead_train': 1.0,\n",
       " 'multiply_embedding_mode': 'sqrt_depth',\n",
       " 'multiproblem_fixed_train_length': -1,\n",
       " 'multiproblem_label_weight': 0.5,\n",
       " 'multiproblem_max_input_length': -1,\n",
       " 'multiproblem_max_target_length': -1,\n",
       " 'multiproblem_mixing_schedule': 'constant',\n",
       " 'multiproblem_per_task_threshold': '',\n",
       " 'multiproblem_reweight_label_loss': False,\n",
       " 'multiproblem_schedule_max_examples': 10000000.0,\n",
       " 'multiproblem_schedule_threshold': 0.5,\n",
       " 'multiproblem_target_eval_only': False,\n",
       " 'multiproblem_vocab_size': -1,\n",
       " 'name': {},\n",
       " 'nbr_decoder_problems': 1,\n",
       " 'no_data_parallelism': False,\n",
       " 'norm_epsilon': 1e-06,\n",
       " 'norm_type': 'layer',\n",
       " 'num_area_layers': 0,\n",
       " 'num_decoder_layers': 0,\n",
       " 'num_encoder_layers': 0,\n",
       " 'num_heads': 8,\n",
       " 'num_hidden_layers': 6,\n",
       " 'optimizer': 'adam',\n",
       " 'optimizer_adafactor_beta1': 0.0,\n",
       " 'optimizer_adafactor_beta2': 0.999,\n",
       " 'optimizer_adafactor_clipping_threshold': 1.0,\n",
       " 'optimizer_adafactor_decay_type': 'pow',\n",
       " 'optimizer_adafactor_factored': True,\n",
       " 'optimizer_adafactor_memory_exponent': 0.8,\n",
       " 'optimizer_adafactor_multiply_by_parameter_scale': True,\n",
       " 'optimizer_adam_beta1': 0.9,\n",
       " 'optimizer_adam_beta2': 0.997,\n",
       " 'optimizer_adam_epsilon': 1e-09,\n",
       " 'optimizer_momentum_momentum': 0.9,\n",
       " 'optimizer_momentum_nesterov': False,\n",
       " 'optimizer_multistep_accumulate_steps': 0,\n",
       " 'optimizer_zero_grads': False,\n",
       " 'overload_eval_metric_name': '',\n",
       " 'pack_dataset': False,\n",
       " 'pad_batch': False,\n",
       " 'parameter_attention_key_channels': 0,\n",
       " 'parameter_attention_value_channels': 0,\n",
       " 'pos': 'timing',\n",
       " 'prepend_mode': 'none',\n",
       " 'pretrained_model_dir': '',\n",
       " 'proximity_bias': False,\n",
       " 'relu_dropout': 0.1,\n",
       " 'relu_dropout_broadcast_dims': '',\n",
       " 'sampling_keep_top_k': -1,\n",
       " 'sampling_method': 'argmax',\n",
       " 'sampling_temp': 1.0,\n",
       " 'scheduled_sampling_gold_mixin_prob': 0.5,\n",
       " 'scheduled_sampling_prob': 0.0,\n",
       " 'scheduled_sampling_warmup_steps': 50000,\n",
       " 'self_attention_type': 'dot_product',\n",
       " 'shared_embedding': False,\n",
       " 'shared_embedding_and_softmax_weights': True,\n",
       " 'split_targets_chunk_length': 0,\n",
       " 'split_targets_max_chunks': 100,\n",
       " 'split_targets_strided_training': False,\n",
       " 'split_to_length': 0,\n",
       " 'summarize_grads': False,\n",
       " 'summarize_vars': False,\n",
       " 'symbol_dropout': 0.0,\n",
       " 'symbol_modality_num_shards': 16,\n",
       " 'top': {},\n",
       " 'tpu_enable_host_call': False,\n",
       " 'unidirectional_encoder': False,\n",
       " 'use_custom_ops': True,\n",
       " 'use_fixed_batch_size': False,\n",
       " 'use_pad_remover': True,\n",
       " 'use_target_space_embedding': True,\n",
       " 'video_num_input_frames': 1,\n",
       " 'video_num_target_frames': 1,\n",
       " 'vocab_divisor': 1,\n",
       " 'warm_start_from_second': '',\n",
       " 'weight_decay': 0.0,\n",
       " 'weight_dtype': 'float32',\n",
       " 'weight_noise': 0.0,\n",
       " 'weights_fn': {}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads(hparams.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Configuring DataParallelism to replicate the model.\n",
      "INFO:tensorflow:schedule=continuous_train_and_eval\n",
      "INFO:tensorflow:worker_gpu=1\n",
      "INFO:tensorflow:sync=False\n",
      "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
      "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
      "INFO:tensorflow:caching_devices: None\n",
      "INFO:tensorflow:ps_devices: ['gpu:0']\n"
     ]
    }
   ],
   "source": [
    "RUN_CONFIG = create_run_config(hparams,\n",
    "                              model_dir=PARAMS['TRAIN_DIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_is_chief': True, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7fcc75648630>, 'use_tpu': False, '_task_id': 0, '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcc756485f8>, '_evaluation_master': '', '_task_type': None, '_log_step_count_steps': 100, '_keep_checkpoint_max': 20, '_model_dir': '/tf/t2t_train', '_eval_distribute': None, '_train_distribute': None, '_save_checkpoints_secs': None, 't2t_device_info': {'num_async_replicas': 1}, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.95\n",
      "}\n",
      "allow_soft_placement: true\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "    global_jit_level: OFF\n",
      "  }\n",
      "}\n",
      "isolate_session_state: true\n",
      ", '_protocol': None, '_keep_checkpoint_every_n_hours': 10000, '_environment': 'local', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_worker_replicas': 0}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7fcc760ab268>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using ValidationMonitor\n",
      "WARNING:tensorflow:EvalSpec not provided. Estimator will not manage model evaluation. Assuming ValidationMonitor present in train_hooks.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Reading data files from /tf/t2t_data/github_function_docstring-train*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 100\n",
      "WARNING:tensorflow:From /tf/tensor2tensor/utils/data_reader.py:275: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "WARNING:tensorflow:From /tf/tensor2tensor/utils/data_reader.py:37: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /tf/tensor2tensor/utils/data_reader.py:233: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'train'\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_8164_512.bottom\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py:1007: calling Graph.create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Shapes are always computed; don't use the compute_shapes as it has no effect.\n",
      "INFO:tensorflow:Transforming feature 'targets' with symbol_modality_8164_512.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "WARNING:tensorflow:From /tf/tensor2tensor/models/transformer.py:93: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_8164_512.top\n",
      "INFO:tensorflow:Base learning rate: 2.000000\n",
      "INFO:tensorflow:Trainable Variables Total size: 48300032\n",
      "INFO:tensorflow:Non-trainable variables Total size: 5\n",
      "INFO:tensorflow:Using optimizer adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /tf/t2t_train/model.ckpt-100000\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 100000 into /tf/t2t_train/model.ckpt.\n",
      "INFO:tensorflow:Reading data files from /tf/t2t_data/github_function_docstring-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 1\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n",
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_8164_512.bottom\n",
      "INFO:tensorflow:Transforming feature 'targets' with symbol_modality_8164_512.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_8164_512.top\n",
      "WARNING:tensorflow:From /tf/tensor2tensor/utils/bleu_hook.py:151: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-07T05:21:13Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tf/t2t_train/model.ckpt-100000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-07-05:22:00\n",
      "INFO:tensorflow:Saving dict for global step 100000: global_step = 100000, loss = 2.4994497, metrics-github_function_docstring/targets/accuracy = 0.51746553, metrics-github_function_docstring/targets/approx_bleu_score = 0.27919203\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100000: /tf/t2t_train/model.ckpt-100000\n",
      "INFO:tensorflow:Validation (step 100001): metrics-github_function_docstring/targets/approx_bleu_score = 0.27919203, global_step = 100000, loss = 2.4994497, metrics-github_function_docstring/targets/accuracy = 0.51746553\n",
      "INFO:tensorflow:loss = 1.9475199, step = 100000\n",
      "INFO:tensorflow:global_step/sec: 1.11382\n",
      "INFO:tensorflow:loss = 1.9239066, step = 100100 (34.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.86311\n",
      "INFO:tensorflow:loss = 2.1250002, step = 100200 (25.885 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.89359\n",
      "INFO:tensorflow:loss = 2.0348961, step = 100300 (25.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.89657\n",
      "INFO:tensorflow:loss = 1.9676901, step = 100400 (25.664 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.91394\n",
      "INFO:tensorflow:loss = 1.999411, step = 100500 (25.549 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-1e00ba0a3fd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0meval_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPARAMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mexp_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tf/tensor2tensor/utils/trainer_lib.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    407\u001b[0m                          \u001b[0;34m\"model evaluation. Assuming ValidationMonitor present \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                          \"in train_hooks.\")\n\u001b[0;32m--> 409\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/tensor2tensor/utils/trainer_lib.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, max_steps)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         max_steps=max_steps or self._train_spec.max_steps)\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_eval_and_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1157\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1405\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m         \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    674\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1169\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1172\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exp_fn = create_experiment(\n",
    "        run_config=RUN_CONFIG,\n",
    "        hparams=hparams,\n",
    "        model_name=PARAMS['T2T_Model'],\n",
    "        problem_name=PARAMS['T2T_Problem'],\n",
    "        data_dir=PARAMS['DATA_DIR'],\n",
    "        train_steps=PARAMS['train_steps'],\n",
    "        eval_steps=PARAMS['eval_steps']\n",
    "    )\n",
    "exp_fn.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ckpt_name = \"model.ckpt\"\n",
    "ckpt_path = tf.train.latest_checkpoint(PARAMS['TRAIN_DIR'])\n",
    "ckpt_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfe = tf.contrib.eager\n",
    "tfe.enable_eager_execution()\n",
    "Modes = tf.estimator.ModeKeys\n",
    "\n",
    "hparams = trainer_lib.create_hparams(PARAMS['T2T_HPARAMS'], data_dir=PARAMS['DATA_DIR'] , problem_name=PARAMS['T2T_Problem'])\n",
    "\n",
    "translate_model = registry.model(PARAMS['T2T_Model'])(hparams, Modes.EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = problems.problem(PARAMS['T2T_Problem']).feature_encoders(PARAMS['DATA_DIR'])\n",
    "\n",
    "def encode(input_str, output_str=None):\n",
    "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "    inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\n",
    "    batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\n",
    "    return {\"inputs\": batch_inputs}\n",
    "\n",
    "def decode(integers):\n",
    "    \"\"\"List of ints to str\"\"\"\n",
    "    integers = list(np.squeeze(integers))\n",
    "    if 1 in integers:\n",
    "        integers = integers[:integers.index(1)]\n",
    "    return encoders[\"inputs\"].decode(np.squeeze(integers))\n",
    "\n",
    "# Restore and translate!\n",
    "def translate(inputs):\n",
    "    encoded_inputs = encode(inputs)\n",
    "    with tfe.restore_variables_on_create(ckpt_path):\n",
    "        model_output = translate_model.infer(encoded_inputs)[\"outputs\"]\n",
    "        return decode(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_path = \"/tf/tensor2tensor/datagen/t2t_datagen\"\n",
    "t = pd.read_csv(os.path.join(data_gen_path, \"func-doc-pairs-00000-of-00100.csv\"), names=['intent', 'code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intent, code = t.iloc[1].intent, t.iloc[1].code\n",
    "intent, code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00000-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00001-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00002-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00003-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00004-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00005-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00006-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00007-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00008-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00009-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00010-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00011-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00012-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00013-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00014-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00015-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00016-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00017-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00018-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00019-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00020-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00021-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00022-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00023-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00024-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00025-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00026-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00027-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00028-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00029-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00030-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00031-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00032-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00033-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00034-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00035-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00036-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00037-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00038-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00039-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00040-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00041-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00042-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00043-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00044-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00045-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00046-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00047-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00048-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00049-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00050-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00051-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00052-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00053-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00054-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00055-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00056-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00057-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00058-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00059-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00060-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00061-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00062-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00063-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00064-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00065-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00066-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00067-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00068-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00069-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00070-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00071-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00072-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00073-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00074-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00075-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00076-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00077-of-00100.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00078-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00079-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00080-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00081-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00082-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00083-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00084-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00085-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00086-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00087-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00088-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00089-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00090-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00091-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00092-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00093-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00094-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00095-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00096-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00097-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00098-of-00100.csv\n",
      "INFO:tensorflow:Not downloading, file already found: /tf/datagen/github/func-doc-pairs-00099-of-00100.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'inputs': 'notifies the end of the procedure operations',\n",
       " 'targets': 'def ProcedureEnd Event Procedure Example RLG finished Severity WARNING Finish Procedure finished return'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensor2tensor.data_generators import function_docstring\n",
    "from tensor2tensor.data_generators import problem\n",
    "\n",
    "g = function_docstring.GithubFunctionDocstring()\n",
    "\n",
    "next(g.generate_samples(PARAMS['DATA_DIR'], PARAMS['TMP_DIR'], problem.DatasetSplit.TRAIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
