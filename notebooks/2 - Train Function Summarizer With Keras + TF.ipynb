{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment is available as a publically available docker container: `hamelsmu/ml-gpu`\n",
    "\n",
    "### Pre-requisite: Familiarize yourself with sequence-to-sequence models\n",
    "\n",
    "If you are not familiar with sequence to sequence models, please refer to [this tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8).\n",
    "\n",
    "### Pre-Requisite: Make Sure you have the right files prepared from Step 1\n",
    "\n",
    "You should have these files in the root of the `./data/processed_data/` directory:\n",
    "\n",
    "1. `{train/valid/test.function}` - these are python function definitions tokenized (by space), 1 line per function.\n",
    "2. `{train/valid/test.docstring}` - these are docstrings that correspond to each of the python function definitions, and have a 1:1 correspondence with the lines in *.function files.\n",
    "3. `{train/valid/test.lineage}` - every line in this file contains a link back to the original location (github repo link) where the code was retrieved.  There is a 1:1 correspondence with the lines in this file and the other two files. This is useful for debugging.\n",
    "\n",
    "\n",
    "### Set the value of `use_cache` appropriately.  \n",
    "\n",
    "If `use_cache = True`, data will be downloaded where possible instead of re-computing.  However, it is highly recommended that you set `use_cache = False` for this tutorial as it will be less confusing, and you will learn more by runing these steps yourself. **This notebook takes approximately 4 hours to run on an AWS `p3.8xlarge` instance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: you can set what GPU you want to use in a notebook like this.  \n",
    "# # Useful if you want to run concurrent experiments at the same time on different GPUs.\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# This will allow the notebook to run faster\n",
    "from pathlib import Path\n",
    "from general_utils import get_step2_prerequisite_files, read_training_files\n",
    "from keras.utils import get_file\n",
    "OUTPUT_PATH = Path('./data/seq2seq/')\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Text From File\n",
    "\n",
    "We want to read in raw text from files so we can pre-process the text for modeling as described in [this tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Num rows for encoder training + validation input: 1,227,989\n",
      "WARNING:root:Num rows for encoder holdout input: 177,220\n",
      "WARNING:root:Num rows for decoder training + validation input: 1,227,989\n",
      "WARNING:root:Num rows for decoder holdout input: 177,220\n"
     ]
    }
   ],
   "source": [
    "# if use_cache:\n",
    "#     get_step2_prerequisite_files(output_directory = './data/processed_data')\n",
    "\n",
    "# you want to supply the directory where the files are from step 1.\n",
    "train_code, holdout_code, train_comment, holdout_comment = read_training_files('./data/processed_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code and comment files should be of the same length.\n",
    "\n",
    "assert len(train_code) == len(train_comment)\n",
    "assert len(holdout_code) == len(holdout_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text\n",
    "\n",
    "In this step, we are going to pre-process the raw text for modeling.  For an explanation of what this section does, see the [Preapre & Clean Data section of this Tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Not fitting transform function because use_cache=True\n"
     ]
    }
   ],
   "source": [
    "from ktext.preprocess import processor\n",
    "import logging\n",
    "\n",
    "if not use_cache:    \n",
    "    code_proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
    "    t_code = code_proc.fit_transform(train_code)\n",
    "\n",
    "    comment_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=14000, padding ='post')\n",
    "    t_comment = comment_proc.fit_transform(train_comment)\n",
    "\n",
    "elif use_cache:\n",
    "    logging.warning('Not fitting transform function because use_cache=True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save tokenized text** (You will reuse this for step 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "if not use_cache:\n",
    "    # Save the preprocessor\n",
    "    with open(OUTPUT_PATH/'py_code_proc_v2.dpkl', 'wb') as f:\n",
    "        dpickle.dump(code_proc, f)\n",
    "\n",
    "    with open(OUTPUT_PATH/'py_comment_proc_v2.dpkl', 'wb') as f:\n",
    "        dpickle.dump(comment_proc, f)\n",
    "\n",
    "    # Save the processed data\n",
    "    np.save(OUTPUT_PATH/'py_t_code_vecs_v2.npy', t_code)\n",
    "    np.save(OUTPUT_PATH/'py_t_comment_vecs_v2.npy', t_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrange data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (1227989, 55)\n",
      "Shape of decoder input: (1227989, 14)\n",
      "Shape of decoder target: (1227989, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for data/seq2seq/py_code_proc_v2.dpkl: 20,002\n",
      "Size of vocabulary for data/seq2seq/py_comment_proc_v2.dpkl: 14,002\n"
     ]
    }
   ],
   "source": [
    "#!cd ./data/seq2seq && wget https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor\n",
    "\n",
    "\n",
    "encoder_input_data, encoder_seq_len = load_encoder_inputs(OUTPUT_PATH/'py_t_code_vecs_v2.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs(OUTPUT_PATH/'py_t_comment_vecs_v2.npy')\n",
    "num_encoder_tokens, enc_pp = load_text_processor(OUTPUT_PATH/'py_code_proc_v2.dpkl')\n",
    "num_decoder_tokens, dec_pp = load_text_processor(OUTPUT_PATH/'py_comment_proc_v2.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have the above files on disk because you set `use_cache = True` you can download the files for the above function calls here:\n",
    "\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_t_code_vecs_v2.npy\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_t_comment_vecs_v2.npy\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Seq2Seq Model For Summarizing Code\n",
    "\n",
    "We will build a model to predict the docstring given a function or a method.  While this is a very cool task in itself, this is not the end goal of this exercise.  The motivation for training this model is to learn a general purpose feature extractor for code that we can use for the task of code search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import build_seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convenience function `build_seq2seq_model` constructs the architecture for a sequence-to-sequence model.  \n",
    "\n",
    "The architecture built for this tutorial is a minimal example with only one layer for the encoder and decoder, and does not include things like [attention](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf).  We encourage you to try and build different architectures to see what works best for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_Model = build_seq2seq_model(word_emb_dim=800,\n",
    "                                    hidden_state_dim=1000,\n",
    "                                    encoder_seq_len=encoder_seq_len,\n",
    "                                    num_encoder_tokens=num_encoder_tokens,\n",
    "                                    num_decoder_tokens=num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 800)    11201600    Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 55)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 800)    3200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 1000)         21407800    Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 1000), 5403000     Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 1000)   4000        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 14002)  14016002    Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 52,035,602\n",
      "Trainable params: 52,030,402\n",
      "Non-trainable params: 5,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "if not use_cache:\n",
    "\n",
    "    from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "    import numpy as np\n",
    "    from keras import optimizers\n",
    "\n",
    "    seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.00005), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    script_name_base = 'py_func_sum_v9_'\n",
    "    csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                       save_best_only=True)\n",
    "\n",
    "    batch_size = 1100\n",
    "    epochs = 16\n",
    "    history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Not re-training function summarizer seq2seq model because use_cache=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for /ds/.keras/datasets/py_code_proc_v2.dpkl: 20,002\n",
      "Size of vocabulary for /ds/.keras/datasets/py_comment_proc_v2.dpkl: 14,002\n"
     ]
    }
   ],
   "source": [
    "if use_cache:\n",
    "    logging.warning('Not re-training function summarizer seq2seq model because use_cache=True')\n",
    "    # Load model from url\n",
    "    loc = get_file(fname='py_func_sum_v9_.epoch16-val2.55276.hdf5',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_func_sum_v9_.epoch16-val2.55276.hdf5')\n",
    "    seq2seq_Model = load_model(loc)\n",
    "    \n",
    "    # Load encoder (code) pre-processor from url\n",
    "    loc = get_file(fname='py_code_proc_v2.dpkl',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl')\n",
    "    num_encoder_tokens, enc_pp = load_text_processor(loc)\n",
    "    \n",
    "    # Load decoder (docstrings/comments) pre-processor from url\n",
    "    loc = get_file(fname='py_comment_proc_v2.dpkl',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl')\n",
    "    num_decoder_tokens, dec_pp = load_text_processor(loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above procedure will automatically download a pre-trained model and associated artifacts from https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/ if `use_cache = True`.  \n",
    "\n",
    "Otherwise, the above code will checkpoint the best model after each epoch into the current directory with prefix `py_func_sum_v9_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Seq2Seq Model For Code Summarization\n",
    "\n",
    "To evaluate this model we are going to do two things:\n",
    "\n",
    "1.  Manually inspect the results of predicted docstrings for code snippets, to make sure they look sensible.\n",
    "2.  Calculate the [BLEU Score](https://en.wikipedia.org/wiki/BLEU) so that we can quantitately benchmark different iterations of this algorithm and to guide hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Inspect Results (on holdout set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 156423 =================\n",
      "\n",
      "Original Input:\n",
      " def testMergelistOutput self ebuilds dev libs A 1 DEPEND dev libs B dev libs C IUSE foo EAPI 1 dev libs B 1 DEPEND dev libs D IUSE foo bar EAPI 1 dev libs C 1 DEPEND dev libs E IUSE foo bar dev libs D 1 IUSE dev libs E 1 dev libs Z 1 IUSE foo EAPI 1 dev libs Y 1 IUSE foo EAPI 1 dev libs X 1 dev libs W 1 IUSE foo EAPI 1 installed dev libs Z 1 USE IUSE foo dev libs Y 1 USE foo IUSE foo EAPI 1 dev libs X 1 USE foo IUSE foo EAPI 1 dev libs W 1 option_cobos verbose tree tree unordered display verbose verbose tree verbose tree unordered display test_cases for options in option_cobos testcase_opts for opt in options testcase_opts opt True test_cases append ResolverPlaygroundTestCase dev libs A options testcase_opts success True ignore_mergelist_order True mergelist dev libs D 1 dev libs E 1 dev libs C 1 dev libs B 1 dev libs A 1 test_cases append ResolverPlaygroundTestCase dev libs Z options testcase_opts success True mergelist dev libs Z 1 test_cases append ResolverPlaygroundTestCase dev libs Y options testcase_opts success True mergelist dev libs Y 1 test_cases append ResolverPlaygroundTestCase dev libs X options testcase_opts success True mergelist dev libs X 1 test_cases append ResolverPlaygroundTestCase dev libs W options testcase_opts success True mergelist dev libs W 1 playground ResolverPlayground ebuilds ebuilds installed installed try for test_case in test_cases playground run_TestCase test_case self assertEqual test_case test_success True test_case fail_msg finally playground cleanup\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"this test does n't check if the output is correct , but makes sure that we do n't backtrace somewhere in the output code .\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " tests that libs are included in the correct libs\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 160940 =================\n",
      "\n",
      "Original Input:\n",
      " def supported_versions django cms cms_version None django_version None try cms_version Decimal cms except ValueError InvalidOperation try cms_version CMS_VERSION_MATRIX str cms except KeyError pass try django_version Decimal django except ValueError InvalidOperation try django_version DJANGO_VERSION_MATRIX str django except KeyError pass try if cms_version and django_version and not LooseVersion VERSION_MATRIX compat unicode cms_version 0 LooseVersion compat unicode django_version LooseVersion VERSION_MATRIX compat unicode cms_version 1 raise RuntimeError Django and django CMS versions doesn t match Django 0 is not supported by django CMS 1 format django_version cms_version except KeyError raise RuntimeError Django and django CMS versions doesn t match Django 0 is not supported by django CMS 1 format django_version cms_version return compat unicode django_version if django_version else django_version compat unicode cms_version if cms_version else cms_version\n",
      " \n",
      "\n",
      "Original Output:\n",
      " convert numeric and literal version information to numeric format\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns a list of supported versions of the given version\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 124719 =================\n",
      "\n",
      "Original Input:\n",
      " def monomial_deg M return sum M\n",
      " \n",
      "\n",
      "Original Output:\n",
      " returns the total degree of a monomial .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns the monomial of the sum of the monomial\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 140950 =================\n",
      "\n",
      "Original Input:\n",
      " def split_assignment stmt_node is_assign isinstance stmt_node Assign is_aug_assign isinstance stmt_node AugAssign store_expr list load_expr None if is_assign or is_aug_assign if is_assign targets stmt_node targets if isinstance targets Tuple or isinstance targets List for elmt in targets elts store_expr append elmt else for elmt in targets store_expr append elmt else store_expr append stmt_node target load_expr stmt_node value return store_expr load_expr\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"takes a `` assign `` or `` augassign `` ast node , and split it into store expressions ( expanded targets ) and value .\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " split a assignment statement into a list of statements\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 138788 =================\n",
      "\n",
      "Original Input:\n",
      " def extract_palette color_palette if isinstance color_palette dict colors list color_palette values if all map ishex colors return colors elif isrgb color_palette return color_palette elif isinstance color_palette list tuple np ndarray if all map ishex color_palette return color_palette elif all map isrgb color_palette return color_palette elif all color in all_colors keys for color in color_palette return all_colors get color for color in color_palette elif isinstance color_palette str color all_colors get color_palette None if color return color elif ishex color_palette return color_palette elif isinstance color_palette type None return color_palette raise ValueError Not a valid color string list or dictionary\n",
      " \n",
      "\n",
      "Original Output:\n",
      " extract color palette information and return a str / list of hex colors .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " extract palette from a palette\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 18899 =================\n",
      "\n",
      "Original Input:\n",
      " abc abstractclassmethod def _insert self req_item kwargs raise NotImplementedError _insert is not implemented\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"the given element is not in the cache , now insert it into cache : param * * kwargs : : param req_item : : return : true on success , false on failure\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " insert a new item into the database\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 36451 =================\n",
      "\n",
      "Original Input:\n",
      " def _format_content self content content re sub n br n content content re sub 160 160 content content re sub 160 160 160 content return content\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"returns ` content ` with consecutive spaces converted to non - break spaces , and linebreak converted into html br elements .\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " format the content of the content\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 28061 =================\n",
      "\n",
      "Original Input:\n",
      " def _create_model self java_model raise NotImplementedError\n",
      " \n",
      "\n",
      "Original Output:\n",
      " creates a model from the input java model reference .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " creates a model for the given model\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 35142 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def _join_parameters base nxt if nxt is None return base if isinstance base set and isinstance nxt set return base nxt else return nxt\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"join parameters from the lhs to the rhs , if compatible .\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " join two or more parameters\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 167836 =================\n",
      "\n",
      "Original Input:\n",
      " def log message args kwargs logging info message format args kwargs\n",
      " \n",
      "\n",
      "Original Output:\n",
      " log the message with a timestamp .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " log a message\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 50065 =================\n",
      "\n",
      "Original Input:\n",
      " def _find_bsb_files root_directory for root _ files in os walk root_directory for basename in fnmatch filter files bsb yield os path join root basename\n",
      " \n",
      "\n",
      "Original Output:\n",
      " walk recursively through subfolders and yield * .bsb files .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " find all files in the directory tree\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 46194 =================\n",
      "\n",
      "Original Input:\n",
      " def testPushEvents self event_heap event_heaps EventHeap self assertEqual len event_heap _heap 0 event1 containers_test_lib TestEvent 5134324321 event2 containers_test_lib TestEvent 2345871286 event_heap PushEvents event1 event2 self assertEqual len event_heap _heap 2\n",
      " \n",
      "\n",
      "Original Output:\n",
      " tests the pushevents function .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test that we can add an event to the heap\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 5553 =================\n",
      "\n",
      "Original Input:\n",
      " def help command str None p command split 1 if len p 1 print HELP command To execute a query relation query If the 1st part is omitted the result will be stored in the relation last_ To prevent from printing the relation append a to the end of the query To insert relational operators type _OPNAME they will be internally replaced with the correct symbol Rember completion is enabled and can be very helpful if you can t remember something return cmd p 1 cmdhelp QUIT Quits the program LIST Lists the relations loaded LOAD LOAD filename relationame Loads a relation into memory UNLOAD UNLOAD relationame Unloads a relation from memory SAVE SAVE filename relationame Saves a relation in a file HELP Prints the help on a command SURVEY Fill and send a survey print cmdhelp get cmd Unknown command s cmd\n",
      " \n",
      "\n",
      "Original Output:\n",
      " prints help on the various functions\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Predicted Output ******:\n",
      " print the help of the command\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 32476 =================\n",
      "\n",
      "Original Input:\n",
      " app route api game game_id methods GET check_valid_request def game_status game Game player Player Dict result Dict game retrieve_game player error_check result return jsonify result\n",
      " \n",
      "\n",
      "Original Output:\n",
      " returns all game info\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return a game status\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 75246 =================\n",
      "\n",
      "Original Input:\n",
      " def teardown_predict_ligpy call rm rf bsub c bsub o ddat in fort 11 f out greg10 in jacobian c jacobian o model c model o net_rates def parest rates def results_dir shell True\n",
      " \n",
      "\n",
      "Original Output:\n",
      " clean up after running predict_ligpy ( ) .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " clean up the test case\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "import pandas as pd\n",
    "\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=enc_pp,\n",
    "                                 decoder_preprocessor=dec_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)\n",
    "\n",
    "demo_testdf = pd.DataFrame({'code':holdout_code, 'comment':holdout_comment, 'ref':''})\n",
    "seq2seq_inf.demo_model_predictions(n=15, df=demo_testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on manual inspection of results:\n",
    "\n",
    "The predicted code summaries are not perfect, but we can see that the model has learned to extract some semantic meaning from the code.  That's all we need to get reasonable results in this case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BLEU Score (on holdout set)\n",
    "\n",
    "BLEU Score is described [in this wikipedia article](https://en.wikipedia.org/wiki/BLEU), and is a way to measure the efficacy of summarization/translation such as the one we conducted here.  This metric is useful if you wish to conduct extensive hyper-parameter tuning and try to improve the seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generating predictions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba4da6216e74659bfa4a158e2b0f36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=177220), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This will return a BLEU Score\n",
    "seq2seq_inf.evaluate_model(input_strings=holdout_code, \n",
    "                           output_strings=holdout_comment, \n",
    "                           max_len=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model to disk\n",
    "\n",
    "Save the model to disk so you can use it in Step 4 of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model_1/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 1000) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.save(OUTPUT_PATH/'code_summary_seq2seq_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
