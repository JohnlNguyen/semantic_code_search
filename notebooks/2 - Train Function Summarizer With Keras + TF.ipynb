{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment is available as a publically available docker container: `hamelsmu/ml-gpu`\n",
    "\n",
    "### Pre-requisite: Familiarize yourself with sequence-to-sequence models\n",
    "\n",
    "If you are not familiar with sequence to sequence models, please refer to [this tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8).\n",
    "\n",
    "### Pre-Requisite: Make Sure you have the right files prepared from Step 1\n",
    "\n",
    "You should have these files in the root of the `./data/processed_data/` directory:\n",
    "\n",
    "1. `{train/valid/test.function}` - these are python function definitions tokenized (by space), 1 line per function.\n",
    "2. `{train/valid/test.docstring}` - these are docstrings that correspond to each of the python function definitions, and have a 1:1 correspondence with the lines in *.function files.\n",
    "3. `{train/valid/test.lineage}` - every line in this file contains a link back to the original location (github repo link) where the code was retrieved.  There is a 1:1 correspondence with the lines in this file and the other two files. This is useful for debugging.\n",
    "\n",
    "\n",
    "### Set the value of `use_cache` appropriately.  \n",
    "\n",
    "If `use_cache = True`, data will be downloaded where possible instead of re-computing.  However, it is highly recommended that you set `use_cache = False` for this tutorial as it will be less confusing, and you will learn more by runing these steps yourself. **This notebook takes approximately 4 hours to run on an AWS `p3.8xlarge` instance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: you can set what GPU you want to use in a notebook like this.  \n",
    "# # Useful if you want to run concurrent experiments at the same time on different GPUs.\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# This will allow the notebook to run faster\n",
    "from pathlib import Path\n",
    "from general_utils import get_step2_prerequisite_files, read_training_files\n",
    "from keras.utils import get_file\n",
    "OUTPUT_PATH = Path('./data/seq2seq/')\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Text From File\n",
    "\n",
    "We want to read in raw text from files so we can pre-process the text for modeling as described in [this tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Num rows for encoder training + validation input: 1,227,989\n",
      "WARNING:root:Num rows for encoder holdout input: 177,220\n",
      "WARNING:root:Num rows for decoder training + validation input: 1,227,989\n",
      "WARNING:root:Num rows for decoder holdout input: 177,220\n"
     ]
    }
   ],
   "source": [
    "# if use_cache:\n",
    "#     get_step2_prerequisite_files(output_directory = './data/processed_data')\n",
    "\n",
    "# you want to supply the directory where the files are from step 1.\n",
    "train_code, holdout_code, train_comment, holdout_comment = read_training_files('./data/processed_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code and comment files should be of the same length.\n",
    "\n",
    "assert len(train_code) == len(train_comment)\n",
    "assert len(holdout_code) == len(holdout_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text\n",
    "\n",
    "In this step, we are going to pre-process the raw text for modeling.  For an explanation of what this section does, see the [Preapre & Clean Data section of this Tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Not fitting transform function because use_cache=True\n"
     ]
    }
   ],
   "source": [
    "from ktext.preprocess import processor\n",
    "import logging\n",
    "\n",
    "if not use_cache:    \n",
    "    code_proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
    "    t_code = code_proc.fit_transform(train_code)\n",
    "\n",
    "    comment_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=14000, padding ='post')\n",
    "    t_comment = comment_proc.fit_transform(train_comment)\n",
    "\n",
    "elif use_cache:\n",
    "    logging.warning('Not fitting transform function because use_cache=True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save tokenized text** (You will reuse this for step 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "if not use_cache:\n",
    "    # Save the preprocessor\n",
    "    with open(OUTPUT_PATH/'py_code_proc_v2.dpkl', 'wb') as f:\n",
    "        dpickle.dump(code_proc, f)\n",
    "\n",
    "    with open(OUTPUT_PATH/'py_comment_proc_v2.dpkl', 'wb') as f:\n",
    "        dpickle.dump(comment_proc, f)\n",
    "\n",
    "    # Save the processed data\n",
    "    np.save(OUTPUT_PATH/'py_t_code_vecs_v2.npy', t_code)\n",
    "    np.save(OUTPUT_PATH/'py_t_comment_vecs_v2.npy', t_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrange data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (1227989, 55)\n",
      "Shape of decoder input: (1227989, 14)\n",
      "Shape of decoder target: (1227989, 14)\n",
      "Size of vocabulary for data/seq2seq/py_code_proc_v2.dpkl: 20,002\n",
      "Size of vocabulary for data/seq2seq/py_comment_proc_v2.dpkl: 14,002\n"
     ]
    }
   ],
   "source": [
    "#!cd ./data/seq2seq && wget https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor\n",
    "\n",
    "\n",
    "encoder_input_data, encoder_seq_len = load_encoder_inputs(OUTPUT_PATH/'py_t_code_vecs_v2.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs(OUTPUT_PATH/'py_t_comment_vecs_v2.npy')\n",
    "num_encoder_tokens, enc_pp = load_text_processor(OUTPUT_PATH/'py_code_proc_v2.dpkl')\n",
    "num_decoder_tokens, dec_pp = load_text_processor(OUTPUT_PATH/'py_comment_proc_v2.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have the above files on disk because you set `use_cache = True` you can download the files for the above function calls here:\n",
    "\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_t_code_vecs_v2.npy\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_t_comment_vecs_v2.npy\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Seq2Seq Model For Summarizing Code\n",
    "\n",
    "We will build a model to predict the docstring given a function or a method.  While this is a very cool task in itself, this is not the end goal of this exercise.  The motivation for training this model is to learn a general purpose feature extractor for code that we can use for the task of code search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import build_seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convenience function `build_seq2seq_model` constructs the architecture for a sequence-to-sequence model.  \n",
    "\n",
    "The architecture built for this tutorial is a minimal example with only one layer for the encoder and decoder, and does not include things like [attention](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf).  We encourage you to try and build different architectures to see what works best for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_Model = build_seq2seq_model(word_emb_dim=800,\n",
    "                                    hidden_state_dim=1000,\n",
    "                                    encoder_seq_len=encoder_seq_len,\n",
    "                                    num_encoder_tokens=num_encoder_tokens,\n",
    "                                    num_decoder_tokens=num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 800)    11201600    Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 55)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 800)    3200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 1000)         21407800    Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 1000), 5403000     Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 1000)   4000        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 14002)  14016002    Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 52,035,602\n",
      "Trainable params: 52,030,402\n",
      "Non-trainable params: 5,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "if not use_cache:\n",
    "\n",
    "    from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "    import numpy as np\n",
    "    from keras import optimizers\n",
    "\n",
    "    seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.00005), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    script_name_base = 'py_func_sum_v9_'\n",
    "    csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                       save_best_only=True)\n",
    "\n",
    "    batch_size = 1100\n",
    "    epochs = 16\n",
    "    history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Not re-training function summarizer seq2seq model because use_cache=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl\n",
      "20824064/20815922 [==============================] - 0s 0us/step\n",
      "Size of vocabulary for /ds/.keras/datasets/py_code_proc_v2.dpkl: 20,002\n",
      "Downloading data from https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl\n",
      "5292032/5288362 [==============================] - 0s 0us/step\n",
      "Size of vocabulary for /ds/.keras/datasets/py_comment_proc_v2.dpkl: 14,002\n"
     ]
    }
   ],
   "source": [
    "if use_cache:\n",
    "    logging.warning('Not re-training function summarizer seq2seq model because use_cache=True')\n",
    "    # Load model from url\n",
    "    loc = get_file(fname='py_func_sum_v9_.epoch16-val2.55276.hdf5',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_func_sum_v9_.epoch16-val2.55276.hdf5')\n",
    "    seq2seq_Model = load_model(loc)\n",
    "    \n",
    "    # Load encoder (code) pre-processor from url\n",
    "    loc = get_file(fname='py_code_proc_v2.dpkl',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl')\n",
    "    num_encoder_tokens, enc_pp = load_text_processor(loc)\n",
    "    \n",
    "    # Load decoder (docstrings/comments) pre-processor from url\n",
    "    loc = get_file(fname='py_comment_proc_v2.dpkl',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl')\n",
    "    num_decoder_tokens, dec_pp = load_text_processor(loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above procedure will automatically download a pre-trained model and associated artifacts from https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/ if `use_cache = True`.  \n",
    "\n",
    "Otherwise, the above code will checkpoint the best model after each epoch into the current directory with prefix `py_func_sum_v9_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Seq2Seq Model For Code Summarization\n",
    "\n",
    "To evaluate this model we are going to do two things:\n",
    "\n",
    "1.  Manually inspect the results of predicted docstrings for code snippets, to make sure they look sensible.\n",
    "2.  Calculate the [BLEU Score](https://en.wikipedia.org/wiki/BLEU) so that we can quantitately benchmark different iterations of this algorithm and to guide hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Inspect Results (on holdout set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 173969 =================\n",
      "\n",
      "Original Input:\n",
      " def pruned_hull_facet_inds hull tol 1e 05 inds ix for ix eq in enumerate hull equations if eq 2 0 tol return inds\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"given a convexhull , return indexes to the facets that make up the bottom of the hull\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return a list of convex hull of a set of vertices\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 58574 =================\n",
      "\n",
      "Original Input:\n",
      " def set_tag self tag tag_len self _tlen r libcrypto EVP_CIPHER_CTX_ctrl self _ctx c_int EVP_CTRL_AEAD_SET_TAG c_int tag_len c_char_p tag if not r self clean raise Exception Set tag failed\n",
      " \n",
      "\n",
      "Original Output:\n",
      " set tag before decrypt any data ( update ) : param tag : authenticated tag : return : none\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " set tag to tag\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 117480 =================\n",
      "\n",
      "Original Input:\n",
      " property abc abstractmethod def num_node_channels self pass\n",
      " \n",
      "\n",
      "Original Output:\n",
      " the number of corresponding channels for each node in the graph .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " number of channels in the node\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 85473 =================\n",
      "\n",
      "Original Input:\n",
      " def test_review_queue_covered self for log_type in mkt LOG_REVIEW_QUEUE if log_type in self blocked continue assert comm ACTION_MAP log_type comm NO_ACTION log_type assert comm ACTION_MAP log_type in comm NOTE_TYPES log_type\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test that every review queue log has its own note type .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test that the queue queue is not supported\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 29447 =================\n",
      "\n",
      "Original Input:\n",
      " def __str__ self Create a multi line ASCII string end of line is n which represents all datas a RootMetadata a author haypo a copyright unicode Hachoir UTF 8 print a Metadata Author haypo Copyright xa9 Hachoir see __unicode__ and exportPlaintext text self exportPlaintext return n join makePrintable line ASCII for line in text\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"create a multi - line ascii string ( end of line is \"\" \\n \"\" ) which represents all datas .\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns a string representation of the object\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 62078 =================\n",
      "\n",
      "Original Input:\n",
      " pytest fixture def err_workbench workbench workbench register CoreManifest workbench register ErrorsManifest workbench register PackagesManifest return workbench\n",
      " \n",
      "\n",
      "Original Output:\n",
      " create a workbench and register basic manifests .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " a canned scenario for the workbench\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 146033 =================\n",
      "\n",
      "Original Input:\n",
      " def convertElementNode elementNode geometryOutput group convertContainerElementNode elementNode geometryOutput Difference\n",
      " \n",
      "\n",
      "Original Output:\n",
      " convert the xml element to a difference xml element .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " add a group to the list of groups\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 100134 =================\n",
      "\n",
      "Original Input:\n",
      " def block_code self code lang code_tag True inlinestyles False linenos False try lexer get_lexer_by_name lang stripnl False except pygments util ClassNotFound lexer get_lexer_by_name text stripnl False if code_tag formatter CodeHtmlFormatter lang noclasses inlinestyles linenos linenos else formatter HtmlFormatter noclasses inlinestyles linenos linenos return highlight code lexer lexer formatter formatter\n",
      " \n",
      "\n",
      "Original Output:\n",
      " render code block .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return code block code\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 88554 =================\n",
      "\n",
      "Original Input:\n",
      " def delete_log self user_id self query DELETE FROM user_changelog WHERE user_id s user_id self _mysql commit\n",
      " \n",
      "\n",
      "Original Output:\n",
      " removes all records from user_changelog related to the user user_id\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " delete a log entry\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 130606 =================\n",
      "\n",
      "Original Input:\n",
      " def test_regex self self assertTrue temp_valid_regex 32f self assertTrue temp_valid_regex 32F self assertTrue temp_valid_regex 32f self assertTrue temp_valid_regex 32F self assertTrue temp_valid_regex 100c self assertTrue temp_valid_regex 100C self assertTrue temp_valid_regex 100c self assertTrue temp_valid_regex 100C self assertTrue temp_valid_regex 32 32f self assertTrue temp_valid_regex 32 442F self assertTrue temp_valid_regex 14 1c self assertTrue temp_valid_regex 33 8889C self assertFalse temp_valid_regex self assertFalse temp_valid_regex C self assertFalse temp_valid_regex F self assertFalse temp_valid_regex 32Ff self assertFalse temp_valid_regex F32 self assertFalse temp_valid_regex F\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test temperature converter 's input - checking regex\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test regex pattern\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 51690 =================\n",
      "\n",
      "Original Input:\n",
      " def run self print Start serving jobs and processing results while not self done self schedule_available_jobs self receive_results print print Signalling end of work to worker processes self work_done_flag set print Waiting for stragglers to hand in results self result_queue join print Wrapping this up self manager shutdown\n",
      " \n",
      "\n",
      "Original Output:\n",
      " this is the actual serving method . it fills the jobqueue and processes incoming results\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " main method of the thread\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 158438 =================\n",
      "\n",
      "Original Input:\n",
      " def controller_for_device device _check_import for controller in CONTROLLERS if controller vendor_id device info vendor and controller product_id device info product return device device constructor controller constructor return None\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"if the evdev inputdevice supplied matches one of our known vendor / product id pairs , return a dict containing ' device'->the device , and ' constructor'->the controller class . if no match is found , return none\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns the controller for the given device\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 134608 =================\n",
      "\n",
      "Original Input:\n",
      " def get_filters_initial self return On 0 Off 0 All len self torrents keys\n",
      " \n",
      "\n",
      "Original Output:\n",
      " that are initial filter tree values .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return the initial filters for the current request\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 121536 =================\n",
      "\n",
      "Original Input:\n",
      " def createDialog self self setWindowTitle s s self tr Release Notes self name self setWindowFlags self windowFlags Qt WindowSystemMenuHint Qt WindowMinMaxButtonsHint layout QVBoxLayout self toRead QLabel s self tr Release notes of the application More details in HISTORY layout addWidget self toRead self rn QTreeWidget self self rn setHeaderHidden True self rn setFrameShape QFrame NoFrame self rn setSelectionMode QAbstractItemView NoSelection self rn setVerticalScrollMode QAbstractItemView ScrollPerPixel self rn setRootIsDecorated False palette QPalette brush QBrush QColor 240 240 240 brush setStyle Qt SolidPattern palette setBrush QPalette Active QPalette Base brush self rn setPalette palette layout addWidget self rn buttonLayout QHBoxLayout buttonLayout addStretch self closeButton QPushButton self tr Close self self closeButton clicked connect self reject buttonLayout addWidget self closeButton layout addLayout buttonLayout self setLayout layout size QSize 700 500 self resize size\n",
      " \n",
      "\n",
      "Original Output:\n",
      " create the qt dialog\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " public method to initialize the dialog\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 52956 =================\n",
      "\n",
      "Original Input:\n",
      " def test_notPresentIfNotSet self self assertIsNone context get x\n",
      " \n",
      "\n",
      "Original Output:\n",
      " arbitrary keys which have not been set in the context have an associated value of l{none}.\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test that the context is returned\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "import pandas as pd\n",
    "\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=enc_pp,\n",
    "                                 decoder_preprocessor=dec_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)\n",
    "\n",
    "demo_testdf = pd.DataFrame({'code':holdout_code, 'comment':holdout_comment, 'ref':''})\n",
    "seq2seq_inf.demo_model_predictions(n=15, df=demo_testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on manual inspection of results:\n",
    "\n",
    "The predicted code summaries are not perfect, but we can see that the model has learned to extract some semantic meaning from the code.  That's all we need to get reasonable results in this case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BLEU Score (on holdout set)\n",
    "\n",
    "BLEU Score is described [in this wikipedia article](https://en.wikipedia.org/wiki/BLEU), and is a way to measure the efficacy of summarization/translation such as the one we conducted here.  This metric is useful if you wish to conduct extensive hyper-parameter tuning and try to improve the seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generating predictions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba4da6216e74659bfa4a158e2b0f36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=177220), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This will return a BLEU Score\n",
    "seq2seq_inf.evaluate_model(input_strings=holdout_code, \n",
    "                           output_strings=holdout_comment, \n",
    "                           max_len=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model to disk\n",
    "\n",
    "Save the model to disk so you can use it in Step 4 of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py:2379: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_2:0' shape=(?, 1000) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.save(OUTPUT_PATH/'code_summary_seq2seq_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
